{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up minmax agent training buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Gets board at next step if agent drops piece in selected column\n",
    "def drop_piece(grid, col, mark, config):\n",
    "    next_grid = grid.copy()\n",
    "    for row in range(config.rows-1, -1, -1):\n",
    "        if next_grid[row][col] == 0:\n",
    "            break\n",
    "    next_grid[row][col] = mark\n",
    "    return next_grid\n",
    "\n",
    "# Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
    "def check_window(window, num_discs, piece, config):\n",
    "    return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
    "    \n",
    "# Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
    "def count_windows(grid, num_discs, piece, config):\n",
    "    num_windows = 0\n",
    "    # horizontal\n",
    "    for row in range(config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[row, col:col+config.inarow])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # vertical\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns):\n",
    "            window = list(grid[row:row+config.inarow, col])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # positive diagonal\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    # negative diagonal\n",
    "    for row in range(config.inarow-1, config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "            if check_window(window, num_discs, piece, config):\n",
    "                num_windows += 1\n",
    "    return num_windows\n",
    "\n",
    "N_STEPS = 2\n",
    "minmax_buff = dict()\n",
    "\n",
    "\n",
    "def get_heuristic(grid, mark, config):\n",
    "    num_threes = count_windows(grid, 3, mark, config)\n",
    "    num_fours = count_windows(grid, 4, mark, config)\n",
    "    num_threes_opp = count_windows(grid, 3, mark%2+1, config)\n",
    "    num_fours_opp = count_windows(grid, 4, mark%2+1, config)\n",
    "    score = num_threes - 1e2*num_threes_opp - 1e4*num_fours_opp + 1e6*num_fours\n",
    "    return score\n",
    "\n",
    "\n",
    "def score_move(grid, col, mark, config, nsteps):\n",
    "    next_grid = drop_piece(grid, col, mark, config)\n",
    "    score = minimax(next_grid, nsteps-1, False, mark, config, -np.Inf, np.Inf)\n",
    "    return score\n",
    "\n",
    "\n",
    "# Helper function for minimax: checks if agent or opponent has four in a row in the window\n",
    "def is_terminal_window(window, config):\n",
    "    return window.count(1) == config.inarow or window.count(2) == config.inarow\n",
    "\n",
    "\n",
    "\n",
    "# Helper function for minimax: checks if game has ended\n",
    "def is_terminal_node(grid, config):\n",
    "    # Check for draw \n",
    "    if list(grid[0, :]).count(0) == 0:\n",
    "        return True\n",
    "    # Check for win: horizontal, vertical, or diagonal\n",
    "    # horizontal \n",
    "    for row in range(config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[row, col:col+config.inarow])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # vertical\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns):\n",
    "            window = list(grid[row:row+config.inarow, col])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # positive diagonal\n",
    "    for row in range(config.rows-(config.inarow-1)):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    # negative diagonal\n",
    "    for row in range(config.inarow-1, config.rows):\n",
    "        for col in range(config.columns-(config.inarow-1)):\n",
    "            window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
    "            if is_terminal_window(window, config):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Minimax implementation\n",
    "def minimax(node, depth, maximizingPlayer, mark, config, alpha, beta):\n",
    "    is_terminal = is_terminal_node(node, config)\n",
    "    valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n",
    "    if depth == 0 or is_terminal:\n",
    "        return get_heuristic(node, mark, config)\n",
    "    \n",
    "    node_lookup = tuple(node.flatten())\n",
    "    \n",
    "    if node_lookup in minmax_buff:\n",
    "        return minmax_buff[node_lookup]\n",
    "    \n",
    "    if maximizingPlayer:\n",
    "        value = -np.Inf\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(node, col, mark, config)\n",
    "            value = max(value, minimax(child, depth-1, False, mark, config, alpha, beta))\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta:\n",
    "                break\n",
    "        minmax_buff[node_lookup] = value\n",
    "        return value\n",
    "    else:\n",
    "        value = np.Inf\n",
    "        for col in valid_moves:\n",
    "            child = drop_piece(node, col, mark%2+1, config)\n",
    "            value = min(value, minimax(child, depth-1, True, mark, config, alpha, beta))\n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        \n",
    "        minmax_buff[node_lookup] = value\n",
    "        return value\n",
    "\n",
    "\n",
    "def minmax_agent(obs, config):\n",
    "    # Get list of valid moves\n",
    "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
    "    # Convert the board to a 2D grid\n",
    "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
    "    # Use the heuristic to assign a score to each possible board in the next step\n",
    "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
    "    # Get a list of columns (moves) that maximize the heuristic\n",
    "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
    "    # Select at random from the maximizing columns\n",
    "    return random.choice(max_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "\n",
    "\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor \n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo.policies import CnnPolicy, MlpPolicy\n",
    "\n",
    "\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2='random'):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "    \n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "        #TODO ADD REWARD FOR CONNECTING MORE AND MORE DOTS \n",
    "    \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "    \n",
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym()\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"log/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Logging progress\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])\n",
    "\n",
    "# Neural network for predicting action values\n",
    "class PyTorchMlp(nn.Module):  \n",
    "\n",
    "    def __init__(self, n_inputs=126, n_actions=7):\n",
    "        nn.Module.__init__(self)\n",
    "\n",
    "        self.fc1 = nn.Linear(n_inputs, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)      \n",
    "        self.fc3 = nn.Linear(64, n_actions)      \n",
    "        self.activ_fn = nn.Tanh()\n",
    "        self.out_activ = nn.Softmax(dim=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activ_fn(self.fc1(x))\n",
    "        x = self.activ_fn(self.fc2(x))\n",
    "        x = self.out_activ(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "# 6x7 input\n",
    "def modified_cnn(scaled_images, **kwargs):  \n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
    "\n",
    "#TODO: try MlpPolicy here\n",
    "# https://www.kaggle.com/c/connectx/discussion/128591\n",
    "class CustomCnnPolicy(MlpPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, activation_fn=nn.Tanh, net_arch=[32, 64, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code cell below to train an agent with PPO and view how the rewards evolved during training.  This code is identical to the code from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "# Try CnnPolicy and MlpPolicy\n",
    "# https://www.kaggle.com/toshikazuwatanabe/connect4-make-submission-with-stable-baselines3/comments\n",
    "#model = PPO('MlpPolicy', vec_env, verbose=2)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reward\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "\n",
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.rolling(window=1000).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_agent(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1), deterministic=True)\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To learn more about the evaluate() function, check out the documentation here: (insert link here)\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(outcomes)\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0.5]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0.5, None]))\n",
    "    print(\"Number of Draws (in {} game rounds):\".format(n_rounds), outcomes.count([0.5, 0.5]))\n",
    "    \n",
    "get_win_percentages('random', dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import zlib\n",
    "import base64 as b64\n",
    "import numpy as np\n",
    "\n",
    "def serializeAndCompress(value, verbose=True):\n",
    "    serializedValue = pickle.dumps(value)\n",
    "    if verbose:\n",
    "        print('Lenght of serialized object:', len(serializedValue))\n",
    "    c_data =  zlib.compress(serializedValue, 9)\n",
    "    if verbose:\n",
    "        print('Lenght of compressed and serialized object:', len(c_data))\n",
    "    return b64.b64encode(c_data)\n",
    "\n",
    "compressed = serializeAndCompress(model)\n",
    "print(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rl.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO1.load('rl.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your agent trained well, the plot (which shows average cumulative rewards) should increase over time.\n",
    "\n",
    "Once you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n",
    "- change `PPO1` to `A2C` (or `ACER` or `ACKTR` or `TRPO`) when defining the model in this line of code: `model = PPO1(CustomCnnPolicy, vec_env, verbose=0)`.  This will let you see how performance can be affected by changing the algorithm from Proximal Policy Optimization [PPO] to one of:\n",
    "  - Advantage Actor-Critic (A2C),\n",
    "  - or Actor-Critic with Experience Replay (ACER),\n",
    "  - Actor Critic using Kronecker-factored Trust Region (ACKTR), or \n",
    "  - Trust Region Policy Optimization (TRPO).\n",
    "- modify the `change_reward()` method in the `ConnectFourGym` class to change the rewards that the agent receives in different conditions.  You may also need to modify `self.reward_range` in the `__init__` method (this tuple should always correspond to the minimum and maximum reward that the agent can receive).\n",
    "- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed the course, and it's time to put your new skills to work!  \n",
    "\n",
    "The next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n",
    "\n",
    "You can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n",
    "\n",
    "As we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS Pytorch",
   "language": "python",
   "name": "ds-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
