{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up training buddies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import matrix_agent, agentc1, agentc2, agentc3, agentc5, agentc7, agentc9, agentc11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the gym environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "from stable_baselines import PPO2 \n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "from common import board_flip\n",
    "from connect4gym import ConnectFourGym\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym([matrix_agent, 'random', agentc1, agentc2, agentc3, agentc5])\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"logtf1/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Logging progress\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.layers import Dropout, BatchNormalization, Dense, Conv2D\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'num_channels': 64,\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 64\n",
    "\n",
    "BN1 = BatchNormalization()\n",
    "BN2 = BatchNormalization()\n",
    "BN3 = BatchNormalization()\n",
    "BN4 = BatchNormalization()\n",
    "BN5 = BatchNormalization()\n",
    "BN6 = BatchNormalization()\n",
    "\n",
    "\n",
    "CONV1 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')\n",
    "CONV2 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')\n",
    "CONV3 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)\n",
    "CONV4 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)\n",
    "\n",
    "FC1 = Dense(128)\n",
    "FC2 = Dense(64)\n",
    "FC3 = Dense(7)\n",
    "\n",
    "DROP1 = Dropout(0.3)\n",
    "DROP2 = Dropout(0.3)\n",
    "\n",
    "\n",
    "# 6x7 input\n",
    "# https://github.com/PaddlePaddle/PARL/blob/0915559a1dd1b9de74ddd2b261e2a4accd0cd96a/benchmark/torch/AlphaZero/submission_template.py#L496\n",
    "def modified_cnn(inputs, **kwargs):\n",
    "    relu = tf.nn.relu\n",
    "    log_softmax = tf.nn.log_softmax\n",
    "    \n",
    "    \n",
    "    layer_1_out = relu(BN1(CONV1(inputs)))\n",
    "    layer_2_out = relu(BN2(CONV2(layer_1_out)))\n",
    "    layer_3_out = relu(BN3(CONV3(layer_2_out)))\n",
    "    layer_4_out = relu(BN4(CONV4(layer_3_out)))\n",
    "    \n",
    "    # 3 is width - 4 due to convolition filters, 2 is same for height\n",
    "    flattened = tf.reshape(layer_4_out, [-1, NUM_CHANNELS * 3 * 2]) \n",
    "    \n",
    "    layer_5_out = DROP1(relu(BN5(FC1(flattened))))\n",
    "    layer_6_out = DROP2(relu(BN6(FC2(layer_5_out))))\n",
    "    \n",
    "    return log_softmax(FC3(layer_6_out))  \n",
    "\n",
    "# https://www.kaggle.com/c/connectx/discussion/128591\n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from connect4gym import SaveBestModelCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code cell below to train an agent with PPO and view how the rewards evolved during training.  This code is identical to the code from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "# Try CnnPolicy and MlpPolicy\n",
    "# https://www.kaggle.com/toshikazuwatanabe/connect4-make-submission-with-stable-baselines3/comments\n",
    "\n",
    "\n",
    "eval_callback = SaveBestModelCallback('RDaneelConnect4_', 1000, ['random', agentc1, agentc3, agentc5, matrix_agent])\n",
    "\n",
    "model = PPO2(CustomCnnPolicy, vec_env, verbose=1)\n",
    "\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=3000, callback=eval_callback)\n",
    "\n",
    "#vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reward\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "\n",
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.rolling(window=100).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def dqn_agent(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    grid = board_flip(obs.mark, np.array(obs['board']).reshape(6,7,1))\n",
    "    col, _ = model.predict(grid, deterministic=True)\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        grid = grid.reshape(6, 7)\n",
    "        #sleep(2)\n",
    "        #print(f'Illegal move attempted! Move: {col}, Boardf:\\n{grid}')\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import get_win_percentages_and_score\n",
    "\n",
    "print('=' * 80)\n",
    "print('VS Random')\n",
    "get_win_percentages_and_score('random', dqn_agent)\n",
    "print('=' * 80)\n",
    "print('VS Heuristic')\n",
    "get_win_percentages_and_score(agentc1, dqn_agent)\n",
    "print('=' * 80)\n",
    "print('VS Minmax2')\n",
    "get_win_percentages_and_score(agentc2, dqn_agent)\n",
    "print('=' * 80)\n",
    "print('VS Minmax3')\n",
    "get_win_percentages_and_score(agentc3, dqn_agent)\n",
    "print('=' * 80)\n",
    "print('VS Minmax5')\n",
    "get_win_percentages_and_score(agentc5, dqn_agent)\n",
    "print('=' * 80)\n",
    "print('VS Minmax7')\n",
    "get_win_percentages_and_score(agentc7, dqn_agent)\n",
    "print('=' * 80)\n",
    "print('VS Matrix Agent')\n",
    "get_win_percentages_and_score(matrix_agent, dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = serializeAndCompress(model.get_parameters())\n",
    "print(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your agent trained well, the plot (which shows average cumulative rewards) should increase over time.\n",
    "\n",
    "Once you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n",
    "- change `PPO1` to `A2C` (or `ACER` or `ACKTR` or `TRPO`) when defining the model in this line of code: `model = PPO1(CustomCnnPolicy, vec_env, verbose=0)`.  This will let you see how performance can be affected by changing the algorithm from Proximal Policy Optimization [PPO] to one of:\n",
    "  - Advantage Actor-Critic (A2C),\n",
    "  - or Actor-Critic with Experience Replay (ACER),\n",
    "  - Actor Critic using Kronecker-factored Trust Region (ACKTR), or \n",
    "  - Trust Region Policy Optimization (TRPO).\n",
    "- modify the `change_reward()` method in the `ConnectFourGym` class to change the rewards that the agent receives in different conditions.  You may also need to modify `self.reward_range` in the `__init__` method (this tuple should always correspond to the minimum and maximum reward that the agent can receive).\n",
    "- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed the course, and it's time to put your new skills to work!  \n",
    "\n",
    "The next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n",
    "\n",
    "You can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n",
    "\n",
    "As we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS TF1 GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
