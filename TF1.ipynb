{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up minmax agent training buddy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport cython\n",
    "from libc.stdlib cimport malloc\n",
    "from libc.stdlib cimport rand, RAND_MAX\n",
    "\n",
    "# Checks for 3 and 4 in a row\n",
    "cdef int get_score(int[42] grid, int mark, int[42] prev_grid, int depth):\n",
    "\n",
    "    cdef int t, row, col, summe\n",
    "    cdef int num_threes = 0\n",
    "    cdef int num_threes_opp = 0\n",
    "     \n",
    "    # horizontal\n",
    "    for row in range(6):\n",
    "        col = 0\n",
    "        while col < 4:\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[row * 7 + col + t]\n",
    "    \n",
    "            if summe < 3 and summe >= 0:\n",
    "                col += 3 - summe\n",
    "                continue\n",
    "                \n",
    "            if summe > -3 and summe < 0:\n",
    "                col += 3 + summe\n",
    "                continue\n",
    "                \n",
    "            col += 1    \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "    \n",
    "    # vertical\n",
    "    for col in range(7):\n",
    "        for row in range(3):\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[(row+t) * 7 + col]\n",
    "            \n",
    "            if summe == 0:\n",
    "                break\n",
    "                \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "\n",
    "    # positive diagonal\n",
    "    for row in range(3):\n",
    "        col = 0\n",
    "        while col < 4:\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[(row+t) * 7 + col + t]\n",
    "    \n",
    "            if summe < 3 and summe >= 0:\n",
    "                col += 3 - summe\n",
    "                continue\n",
    "                \n",
    "            if summe > -3 and summe < 0:\n",
    "                col += 3 + summe\n",
    "                continue\n",
    "                \n",
    "            col += 1   \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "\n",
    "    # negative diagonal\n",
    "    for row in range(3,6):\n",
    "        col = 0\n",
    "        while col < 4:\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[(row-t) * 7 + col + t]\n",
    "    \n",
    "            if summe < 3 and summe >= 0:\n",
    "                col += 3 - summe\n",
    "                continue\n",
    "                \n",
    "            if summe > -3 and summe < 0:\n",
    "                col += 3 + summe\n",
    "                continue\n",
    "                \n",
    "            col += 1   \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "                  \n",
    "    return num_threes - 2 * num_threes_opp # Alternatively weigh opponents higher or lower\n",
    "\n",
    "\n",
    "# Checks if it is a terminal position, if true it returns the score\n",
    "cdef int is_terminal_node(int[42] board, int column, int mark, int row, int player_mark, int depth):\n",
    "    \n",
    "    cdef int i = 0\n",
    "    cdef int j = 0\n",
    "    cdef int col = 0\n",
    "    \n",
    "    # To check if board is full\n",
    "    for col in range(7):\n",
    "        if board[col] == 0:\n",
    "            break\n",
    "        col += 1\n",
    "    \n",
    "    # vertical\n",
    "    if row < 3:\n",
    "        for i in range(1, 4):\n",
    "            if board[column + (row+i) * 7] != mark:\n",
    "                break\n",
    "            i += 1\n",
    "    if i == 4:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth # depth added, so that it chooses the faster option to win\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    # horizontal\n",
    "    for i in range(1, 4):\n",
    "        if (column + i) >= 7 or board[column + i + (row) * 7] != mark:\n",
    "            break\n",
    "        i += 1\n",
    "    for j in range(1, 4):\n",
    "        if (column - j) < 0 or board[column - j + (row) * 7] != mark:\n",
    "            break\n",
    "        j += 1\n",
    "    if (i + j) >= 5:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    # top left diagonal\n",
    "    for i in range(1, 4):\n",
    "        if (column + i) >= 7 or (row + i) >= 6 or board[column + i + (row + i) * 7] != mark:\n",
    "            break\n",
    "        i += 1\n",
    "    for j in range(1, 4):\n",
    "        if (column - j) < 0 or(row - j) < 0 or board[column - j + (row - j) * 7] != mark:\n",
    "            break\n",
    "        j += 1\n",
    "    if (i + j) >= 5:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    # top right diagonal\n",
    "    for i in range(1, 4):\n",
    "        if (column + i) >= 7 or (row - i) < 0 or board[column + i + (row - i) * 7] != mark:\n",
    "            break\n",
    "        i += 1\n",
    "    for j in range(1, 4):\n",
    "        if (column - j) < 0 or(row + j) >= 6 or board[column - j + (row + j) * 7] != mark:\n",
    "            break\n",
    "        j += 1\n",
    "    if (i + j) >= 5:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    if col == 7:\n",
    "        return 1 # draw\n",
    "    return 0 # nobody has won so far\n",
    "\n",
    "\n",
    "# Initial move is scored with minimax\n",
    "cdef int score_move(int[42] grid, int col, int mark, int nsteps):\n",
    "\n",
    "    cdef int[42] next_grid = grid\n",
    "    cdef int row, row2, column\n",
    "    cdef int[42] child\n",
    "    \n",
    "    for row in range(5, -1, -1):\n",
    "        if next_grid[7 * row + col] == 0:\n",
    "            next_grid[7 * row + col] = mark # drop mark\n",
    "            break\n",
    "    \n",
    "    if nsteps > 2: # check if there is an obvious move\n",
    "        is_terminal = is_terminal_node(next_grid, col, mark, row, mark, nsteps-1)\n",
    "        if is_terminal != 0:\n",
    "            return is_terminal\n",
    "\n",
    "        for column in range(7):\n",
    "            if next_grid[column] != 0:\n",
    "                continue\n",
    "            child = next_grid\n",
    "            for row2 in range(5, -1, -1):\n",
    "                if child[7 * row2 + column] == 0:\n",
    "                    child[7 * row2 + column] = mark*(-1)\n",
    "                    break\n",
    "\n",
    "            is_terminal = is_terminal_node(child, column, mark*(-1), row2, mark, nsteps-2)\n",
    "            if is_terminal != 0:\n",
    "                return is_terminal + (col == column) #added in case the opponent makes a mistake\n",
    "        \n",
    "    cdef int alpha = - 10000000\n",
    "    cdef int beta = 10000000\n",
    "    return minimax(next_grid, nsteps-1, 0, mark, grid, alpha, beta, col, row)\n",
    "\n",
    "\n",
    "# Minimax agent with alpha-beta pruning\n",
    "cdef int minimax(int[42] node, int depth, int maximizingPlayer, int mark, int[42] grid, int alpha, int beta, int column, int newrow):\n",
    "    \n",
    "    cdef int is_terminal \n",
    "    if maximizingPlayer:\n",
    "        is_terminal = is_terminal_node(node, column, mark*(-1), newrow, mark, depth)\n",
    "        if is_terminal != 0:\n",
    "            return is_terminal\n",
    "    if maximizingPlayer == 0:\n",
    "        is_terminal = is_terminal_node(node, column, mark, newrow, mark, depth)\n",
    "        if is_terminal != 0:\n",
    "            return is_terminal\n",
    "\n",
    "    cdef int value, col, row\n",
    "    cdef int[42] child\n",
    "    \n",
    "    if depth == 0:\n",
    "        return get_score(node, mark, grid, depth)\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        value = -1000000\n",
    "        for col in range(7):\n",
    "            if node[col] != 0:\n",
    "                continue\n",
    "            child = node\n",
    "            for row in range(5, -1, -1):\n",
    "                if child[7 * row + col] == 0:\n",
    "                    child[7 * row + col] = mark \n",
    "                    break\n",
    "            value = max(value, minimax(child, depth-1, 0, mark, grid, alpha, beta, col, row))\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta:\n",
    "                break\n",
    "        return value\n",
    "    else:\n",
    "        value = 1000000\n",
    "        for col in range(7):\n",
    "            if node[col] != 0:\n",
    "                continue\n",
    "            child = node\n",
    "            for row in range(5, -1, -1):\n",
    "                if child[7 * row + col] == 0:\n",
    "                    child[7 * row + col] = mark*(-1)\n",
    "                    break\n",
    "            value = min(value, minimax(child, depth-1, 1, mark, grid, alpha, beta, col, row))\n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return value\n",
    "    \n",
    "\n",
    "# define the agent   \n",
    "@cython.cdivision(True)\n",
    "cpdef int agen(list grid, int mark, int N_STEPS):\n",
    "    \n",
    "    if mark == 2:\n",
    "        mark = -1\n",
    "        \n",
    "    cdef int num_max = 1\n",
    "    cdef int col, sc, i\n",
    "    cdef int maxsc = -1000001\n",
    "    cdef int[7] score = [-10000, -10000, -10000, -10000, -10000, -10000, -10000]\n",
    "\n",
    "    cdef int *c_grid\n",
    "    \n",
    "    c_grid = <int *>malloc(42*cython.sizeof(int))\n",
    "    for i in range(42):\n",
    "        if grid[i] == 2:\n",
    "            c_grid[i] = -1\n",
    "            continue\n",
    "        c_grid[i] = grid[i]\n",
    "    \n",
    "    for col in range(7):\n",
    "        if c_grid[col] == 0:\n",
    "            sc = score_move(c_grid, col, mark, N_STEPS)\n",
    "            if sc == maxsc:\n",
    "                num_max += 1\n",
    "                \n",
    "            if sc > maxsc:\n",
    "                maxsc = sc\n",
    "                num_max = 1\n",
    "                \n",
    "            score[col] = sc\n",
    "            \n",
    "    cdef int choice = int(rand()/(RAND_MAX/num_max))\n",
    "    cdef int indx = 0\n",
    "    \n",
    "    #print(score, mark)\n",
    "\n",
    "    for i in range(7):\n",
    "        if score[i] == maxsc:\n",
    "            if choice == indx:\n",
    "                return i  \n",
    "            indx += 1\n",
    "     \n",
    "    return 0 # shouldn't be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentc1(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 1)\n",
    "\n",
    "def agentc2(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 2)\n",
    "\n",
    "def agentc3(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 3)\n",
    "\n",
    "def agentc5(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 5)\n",
    "\n",
    "def agentc7(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 7)\n",
    "\n",
    "def agentc9(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 9)\n",
    "\n",
    "def agentc11(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "from stable_baselines import PPO2 \n",
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, agent2='random'):\n",
    "        ks_env = make(\"connectx\", debug=True)\n",
    "        self.env = ks_env.train([None, agent2])\n",
    "        self.rows = ks_env.configuration.rows\n",
    "        self.columns = ks_env.configuration.columns\n",
    "        self.inarow = ks_env.configuration.inarow\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "        self.last_twos = 0\n",
    "        self.last_threes = 0\n",
    "        self.last_action = -1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.obs = self.env.reset()\n",
    "        self.last_twos = 0\n",
    "        self.last_threes = 0\n",
    "        self.last_action = -1\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
    "    \n",
    "    def check_window(self, window, num_discs, piece):\n",
    "        return (window.count(piece) == num_discs)# and window.count(0) == self.inarow-num_discs)\n",
    "    \n",
    "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
    "    def count_windows(self, grid, num_discs, piece):\n",
    "        num_windows = 0\n",
    "        # horizontal\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.columns-(self.inarow-1)):\n",
    "                window = list(grid[row, col:col+self.inarow])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "        # vertical\n",
    "        for row in range(self.rows-(self.inarow-1)):\n",
    "            for col in range(self.columns):\n",
    "                window = list(grid[row:row+self.inarow, col])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "        # positive diagonal\n",
    "        for row in range(self.rows-(self.inarow-1)):\n",
    "            for col in range(self.columns-(self.inarow-1)):\n",
    "                window = list(grid[range(row, row+self.inarow), range(col, col+self.inarow)])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "        # negative diagonal\n",
    "        for row in range(self.inarow-1, self.rows):\n",
    "            for col in range(self.columns-(self.inarow-1)):\n",
    "                window = list(grid[range(row, row-self.inarow, -1), range(col, col+self.inarow)])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "                    \n",
    "        return num_windows\n",
    "    \n",
    "    def calculate_heuristic_reward(self):\n",
    "        grid = np.asarray(self.obs.board).reshape(self.rows, self.columns)\n",
    "        twos = self.count_windows(grid, 2, self.obs.mark)\n",
    "        threes = self.count_windows(grid, 3, self.obs.mark)\n",
    "        \n",
    "        reward = (twos - self.last_twos) * 1 + (threes - self.last_threes) * 3\n",
    "        if reward < 0:\n",
    "            raise AssertionError(f'\\nmark:   {self.obs.mark}   last_action: {self.last_action}\\n' + \n",
    "                                 f'twos:   {twos}   last_twos: {self.last_twos}\\n' + \n",
    "                                 f'threes: {threes} last_threes: {self.last_threes}\\n' +\n",
    "                                 f'Board:  \\n{np.array_str(grid)}')\n",
    "            \n",
    "        self.last_twos = twos\n",
    "        self.last_threes = threes\n",
    "        \n",
    "        return reward\n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "    \n",
    "\"\"\"    \n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 100\n",
    "        elif done: # The opponent won the game\n",
    "            return -100\n",
    "        else: # Reward 10/42\n",
    "            still_alive_reward = 10 / (self.rows*self.columns)\n",
    "            #move_reward = self.calculate_heuristic_reward()\n",
    "            return still_alive_reward# + move_reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        self.last_action = action\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -1000, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym(agentc1)\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"logtf/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Logging progress\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])\n",
    "\n",
    "# 6x7 input\n",
    "def modified_cnn(scaled_images, **kwargs):\n",
    "    activ = tf.nn.relu\n",
    "    \n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=128, filter_size=5, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=256, filter_size=2, stride=1, \n",
    "                         init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = conv_to_fc(layer_2)\n",
    "    return activ(linear(layer_2, 'fc1', n_hidden=4096, init_scale=np.sqrt(2)))  \n",
    "\n",
    "# https://www.kaggle.com/c/connectx/discussion/128591\n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code cell below to train an agent with PPO and view how the rewards evolved during training.  This code is identical to the code from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "# Try CnnPolicy and MlpPolicy\n",
    "# https://www.kaggle.com/toshikazuwatanabe/connect4-make-submission-with-stable-baselines3/comments\n",
    "model = PPO2(CustomCnnPolicy, vec_env, verbose=1)\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reward\n",
    "\n",
    "#plt.figure(figsize = (20,20))\n",
    "\n",
    "\n",
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.rolling(window=1000).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def dqn_agent(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    grid = np.array(obs['board']).reshape(6,7,1)\n",
    "    col, _ = model.predict(grid, deterministic=True)\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        grid = grid.reshape(6, 7)\n",
    "        sleep(2)\n",
    "        #print(f'Illegal move attempted! Move: {col}, Boardf:\\n{grid}')\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n",
    "    \n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.play([dqn_agent, None], width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To learn more about the evaluate() function, check out the documentation here: (insert link here)\n",
    "def get_win_percentages(agent1, agent2, n_rounds=100):\n",
    "    # Use default Connect Four setup\n",
    "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
    "    # Agent 1 goes first (roughly) half the time          \n",
    "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
    "    # Agent 2 goes first (roughly) half the time      \n",
    "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
    "    print(outcomes)\n",
    "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
    "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
    "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
    "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n",
    "    print(\"Number of Draws (in {} game rounds):\".format(n_rounds), outcomes.count([0, 0]))\n",
    "    \n",
    "get_win_percentages('random', dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agentc1, dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agentc2, dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = ConnectFourGym(agentc2)\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"logtf/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Logging progress\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])\n",
    "\n",
    "model.env = vec_env\n",
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agentc1, dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages(agentc2, dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages('random', dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import zlib\n",
    "import base64 as b64\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def serializeAndCompress(value, verbose=True):\n",
    "    serializedValue = pickle.dumps(value)\n",
    "    if verbose:\n",
    "        print('Lenght of serialized object:', len(serializedValue))\n",
    "    c_data =  zlib.compress(serializedValue, 9)\n",
    "    if verbose:\n",
    "        print('Lenght of compressed and serialized object:', len(c_data))\n",
    "    return b64.b64encode(c_data)\n",
    "\n",
    "compressed = serializeAndCompress(model)\n",
    "print(compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('rl.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO1.load('rl.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your agent trained well, the plot (which shows average cumulative rewards) should increase over time.\n",
    "\n",
    "Once you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n",
    "- change `PPO1` to `A2C` (or `ACER` or `ACKTR` or `TRPO`) when defining the model in this line of code: `model = PPO1(CustomCnnPolicy, vec_env, verbose=0)`.  This will let you see how performance can be affected by changing the algorithm from Proximal Policy Optimization [PPO] to one of:\n",
    "  - Advantage Actor-Critic (A2C),\n",
    "  - or Actor-Critic with Experience Replay (ACER),\n",
    "  - Actor Critic using Kronecker-factored Trust Region (ACKTR), or \n",
    "  - Trust Region Policy Optimization (TRPO).\n",
    "- modify the `change_reward()` method in the `ConnectFourGym` class to change the rewards that the agent receives in different conditions.  You may also need to modify `self.reward_range` in the `__init__` method (this tuple should always correspond to the minimum and maximum reward that the agent can receive).\n",
    "- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed the course, and it's time to put your new skills to work!  \n",
    "\n",
    "The next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n",
    "\n",
    "You can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n",
    "\n",
    "As we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS TF1 GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
