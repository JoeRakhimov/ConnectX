{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up training buddies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "cimport cython\n",
    "from libc.stdlib cimport malloc\n",
    "from libc.stdlib cimport rand, RAND_MAX\n",
    "\n",
    "# Checks for 3 and 4 in a row\n",
    "cdef int get_score(int[42] grid, int mark, int[42] prev_grid, int depth):\n",
    "\n",
    "    cdef int t, row, col, summe\n",
    "    cdef int num_threes = 0\n",
    "    cdef int num_threes_opp = 0\n",
    "     \n",
    "    # horizontal\n",
    "    for row in range(6):\n",
    "        col = 0\n",
    "        while col < 4:\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[row * 7 + col + t]\n",
    "    \n",
    "            if summe < 3 and summe >= 0:\n",
    "                col += 3 - summe\n",
    "                continue\n",
    "                \n",
    "            if summe > -3 and summe < 0:\n",
    "                col += 3 + summe\n",
    "                continue\n",
    "                \n",
    "            col += 1    \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "    \n",
    "    # vertical\n",
    "    for col in range(7):\n",
    "        for row in range(3):\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[(row+t) * 7 + col]\n",
    "            \n",
    "            if summe == 0:\n",
    "                break\n",
    "                \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "\n",
    "    # positive diagonal\n",
    "    for row in range(3):\n",
    "        col = 0\n",
    "        while col < 4:\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[(row+t) * 7 + col + t]\n",
    "    \n",
    "            if summe < 3 and summe >= 0:\n",
    "                col += 3 - summe\n",
    "                continue\n",
    "                \n",
    "            if summe > -3 and summe < 0:\n",
    "                col += 3 + summe\n",
    "                continue\n",
    "                \n",
    "            col += 1   \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "\n",
    "    # negative diagonal\n",
    "    for row in range(3,6):\n",
    "        col = 0\n",
    "        while col < 4:\n",
    "            summe = 0\n",
    "            for t in range(4):\n",
    "                summe += grid[(row-t) * 7 + col + t]\n",
    "    \n",
    "            if summe < 3 and summe >= 0:\n",
    "                col += 3 - summe\n",
    "                continue\n",
    "                \n",
    "            if summe > -3 and summe < 0:\n",
    "                col += 3 + summe\n",
    "                continue\n",
    "                \n",
    "            col += 1   \n",
    "            summe *= mark\n",
    "            if summe == 3:\n",
    "                num_threes += 1\n",
    "                continue\n",
    "            if summe == -3:\n",
    "                num_threes_opp += 1\n",
    "                  \n",
    "    return num_threes - 2 * num_threes_opp # Alternatively weigh opponents higher or lower\n",
    "\n",
    "\n",
    "# Checks if it is a terminal position, if true it returns the score\n",
    "cdef int is_terminal_node(int[42] board, int column, int mark, int row, int player_mark, int depth):\n",
    "    \n",
    "    cdef int i = 0\n",
    "    cdef int j = 0\n",
    "    cdef int col = 0\n",
    "    \n",
    "    # To check if board is full\n",
    "    for col in range(7):\n",
    "        if board[col] == 0:\n",
    "            break\n",
    "        col += 1\n",
    "    \n",
    "    # vertical\n",
    "    if row < 3:\n",
    "        for i in range(1, 4):\n",
    "            if board[column + (row+i) * 7] != mark:\n",
    "                break\n",
    "            i += 1\n",
    "    if i == 4:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth # depth added, so that it chooses the faster option to win\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    # horizontal\n",
    "    for i in range(1, 4):\n",
    "        if (column + i) >= 7 or board[column + i + (row) * 7] != mark:\n",
    "            break\n",
    "        i += 1\n",
    "    for j in range(1, 4):\n",
    "        if (column - j) < 0 or board[column - j + (row) * 7] != mark:\n",
    "            break\n",
    "        j += 1\n",
    "    if (i + j) >= 5:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    # top left diagonal\n",
    "    for i in range(1, 4):\n",
    "        if (column + i) >= 7 or (row + i) >= 6 or board[column + i + (row + i) * 7] != mark:\n",
    "            break\n",
    "        i += 1\n",
    "    for j in range(1, 4):\n",
    "        if (column - j) < 0 or(row - j) < 0 or board[column - j + (row - j) * 7] != mark:\n",
    "            break\n",
    "        j += 1\n",
    "    if (i + j) >= 5:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    # top right diagonal\n",
    "    for i in range(1, 4):\n",
    "        if (column + i) >= 7 or (row - i) < 0 or board[column + i + (row - i) * 7] != mark:\n",
    "            break\n",
    "        i += 1\n",
    "    for j in range(1, 4):\n",
    "        if (column - j) < 0 or(row + j) >= 6 or board[column - j + (row + j) * 7] != mark:\n",
    "            break\n",
    "        j += 1\n",
    "    if (i + j) >= 5:\n",
    "        if player_mark == mark:\n",
    "            return 1000 + depth\n",
    "        else:\n",
    "            return -1000 - depth\n",
    "    \n",
    "    if col == 7:\n",
    "        return 1 # draw\n",
    "    return 0 # nobody has won so far\n",
    "\n",
    "\n",
    "# Initial move is scored with minimax\n",
    "cdef int score_move(int[42] grid, int col, int mark, int nsteps):\n",
    "\n",
    "    cdef int[42] next_grid = grid\n",
    "    cdef int row, row2, column\n",
    "    cdef int[42] child\n",
    "    \n",
    "    for row in range(5, -1, -1):\n",
    "        if next_grid[7 * row + col] == 0:\n",
    "            next_grid[7 * row + col] = mark # drop mark\n",
    "            break\n",
    "    \n",
    "    if nsteps > 2: # check if there is an obvious move\n",
    "        is_terminal = is_terminal_node(next_grid, col, mark, row, mark, nsteps-1)\n",
    "        if is_terminal != 0:\n",
    "            return is_terminal\n",
    "\n",
    "        for column in range(7):\n",
    "            if next_grid[column] != 0:\n",
    "                continue\n",
    "            child = next_grid\n",
    "            for row2 in range(5, -1, -1):\n",
    "                if child[7 * row2 + column] == 0:\n",
    "                    child[7 * row2 + column] = mark*(-1)\n",
    "                    break\n",
    "\n",
    "            is_terminal = is_terminal_node(child, column, mark*(-1), row2, mark, nsteps-2)\n",
    "            if is_terminal != 0:\n",
    "                return is_terminal + (col == column) #added in case the opponent makes a mistake\n",
    "        \n",
    "    cdef int alpha = - 10000000\n",
    "    cdef int beta = 10000000\n",
    "    return minimax(next_grid, nsteps-1, 0, mark, grid, alpha, beta, col, row)\n",
    "\n",
    "\n",
    "# Minimax agent with alpha-beta pruning\n",
    "cdef int minimax(int[42] node, int depth, int maximizingPlayer, int mark, int[42] grid, int alpha, int beta, int column, int newrow):\n",
    "    \n",
    "    cdef int is_terminal \n",
    "    if maximizingPlayer:\n",
    "        is_terminal = is_terminal_node(node, column, mark*(-1), newrow, mark, depth)\n",
    "        if is_terminal != 0:\n",
    "            return is_terminal\n",
    "    if maximizingPlayer == 0:\n",
    "        is_terminal = is_terminal_node(node, column, mark, newrow, mark, depth)\n",
    "        if is_terminal != 0:\n",
    "            return is_terminal\n",
    "\n",
    "    cdef int value, col, row\n",
    "    cdef int[42] child\n",
    "    \n",
    "    if depth == 0:\n",
    "        return get_score(node, mark, grid, depth)\n",
    "\n",
    "    if maximizingPlayer:\n",
    "        value = -1000000\n",
    "        for col in range(7):\n",
    "            if node[col] != 0:\n",
    "                continue\n",
    "            child = node\n",
    "            for row in range(5, -1, -1):\n",
    "                if child[7 * row + col] == 0:\n",
    "                    child[7 * row + col] = mark \n",
    "                    break\n",
    "            value = max(value, minimax(child, depth-1, 0, mark, grid, alpha, beta, col, row))\n",
    "            alpha = max(alpha, value)\n",
    "            if alpha >= beta:\n",
    "                break\n",
    "        return value\n",
    "    else:\n",
    "        value = 1000000\n",
    "        for col in range(7):\n",
    "            if node[col] != 0:\n",
    "                continue\n",
    "            child = node\n",
    "            for row in range(5, -1, -1):\n",
    "                if child[7 * row + col] == 0:\n",
    "                    child[7 * row + col] = mark*(-1)\n",
    "                    break\n",
    "            value = min(value, minimax(child, depth-1, 1, mark, grid, alpha, beta, col, row))\n",
    "            beta = min(beta, value)\n",
    "            if beta <= alpha:\n",
    "                break\n",
    "        return value\n",
    "    \n",
    "\n",
    "# define the agent   \n",
    "@cython.cdivision(True)\n",
    "cpdef int agen(list grid, int mark, int N_STEPS):\n",
    "    \n",
    "    if mark == 2:\n",
    "        mark = -1\n",
    "        \n",
    "    cdef int num_max = 1\n",
    "    cdef int col, sc, i\n",
    "    cdef int maxsc = -1000001\n",
    "    cdef int[7] score = [-10000, -10000, -10000, -10000, -10000, -10000, -10000]\n",
    "\n",
    "    cdef int *c_grid\n",
    "    \n",
    "    c_grid = <int *>malloc(42*cython.sizeof(int))\n",
    "    for i in range(42):\n",
    "        if grid[i] == 2:\n",
    "            c_grid[i] = -1\n",
    "            continue\n",
    "        c_grid[i] = grid[i]\n",
    "    \n",
    "    for col in range(7):\n",
    "        if c_grid[col] == 0:\n",
    "            sc = score_move(c_grid, col, mark, N_STEPS)\n",
    "            if sc == maxsc:\n",
    "                num_max += 1\n",
    "                \n",
    "            if sc > maxsc:\n",
    "                maxsc = sc\n",
    "                num_max = 1\n",
    "                \n",
    "            score[col] = sc\n",
    "            \n",
    "    cdef int choice = int(rand()/(RAND_MAX/num_max))\n",
    "    cdef int indx = 0\n",
    "    \n",
    "    #print(score, mark)\n",
    "\n",
    "    for i in range(7):\n",
    "        if score[i] == maxsc:\n",
    "            if choice == indx:\n",
    "                return i  \n",
    "            indx += 1\n",
    "     \n",
    "    return 0 # shouldn't be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import matrix_agent\n",
    "\n",
    "def agentc1(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 1)\n",
    "\n",
    "def agentc2(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 2)\n",
    "\n",
    "def agentc3(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 3)\n",
    "\n",
    "def agentc5(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 5)\n",
    "\n",
    "def agentc7(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 7)\n",
    "\n",
    "def agentc9(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 9)\n",
    "\n",
    "def agentc11(obs, conf):\n",
    "    return agen(obs.board, obs.mark, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the gym environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from kaggle_environments import make, evaluate\n",
    "from gym import spaces\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from stable_baselines.bench import Monitor \n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "\n",
    "from stable_baselines import PPO2 \n",
    "from stable_baselines.common.policies import CnnPolicy\n",
    "from stable_baselines.common.callbacks import EvalCallback\n",
    "\n",
    "from numpy.random import choice\n",
    "\n",
    "\n",
    "# We are always playing mark 1\n",
    "def board_flip(mark, board):\n",
    "    if mark == 1:\n",
    "        return board\n",
    "\n",
    "    for i in range(board.shape[0]):\n",
    "        for j in range(board.shape[1]):\n",
    "            if board[i, j, 0] != 0:\n",
    "                board[i, j, 0] = board[i, j, 0]%2 + 1\n",
    "\n",
    "    return board\n",
    "\n",
    "class ConnectFourGym:\n",
    "    def __init__(self, opponent_pool=np.asarray(['random']), distribution='even'):\n",
    "        self.ks_env = make(\"connectx\", debug=True)\n",
    "        self.rows = self.ks_env.configuration.rows\n",
    "        self.columns = self.ks_env.configuration.columns\n",
    "        self.inarow = self.ks_env.configuration.inarow\n",
    "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
    "        self.action_space = spaces.Discrete(self.columns)\n",
    "        self.observation_space = spaces.Box(low=0, high=2, \n",
    "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
    "        # Tuple corresponding to the min and max possible rewards\n",
    "        self.reward_range = (-10, 1)\n",
    "        # StableBaselines throws error if these are not defined\n",
    "        self.spec = None\n",
    "        self.metadata = None\n",
    "        self.last_action = -1\n",
    "        self.iter = 0\n",
    "        self.opponent_pool = opponent_pool\n",
    "        self.distribution = distribution\n",
    "        self.init_env()\n",
    "\n",
    "    \n",
    "    def init_env(self):\n",
    "        if self.distribution == 'even':\n",
    "            distribution = [1.0 / len(self.opponent_pool)] * len(self.opponent_pool)\n",
    "        else:\n",
    "            distribution = self.distribution\n",
    "            \n",
    "        opponent = choice(self.opponent_pool, 1, p=distribution)[0]\n",
    "        \n",
    "        if self.iter % 2:\n",
    "            self.env = self.ks_env.train([None, opponent])\n",
    "        else:\n",
    "            self.env = self.ks_env.train([opponent, None])\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        self.iter += 1\n",
    "        self.init_env()\n",
    "        self.obs = self.env.reset()\n",
    "        self.last_action = -1\n",
    "        return board_flip(self.obs.mark, np.array(self.obs['board']).reshape(self.rows,self.columns,1))\n",
    "    \n",
    "    def check_window(self, window, num_discs, piece):\n",
    "        return (window.count(piece) == num_discs)# and window.count(0) == self.inarow-num_discs)\n",
    "    \n",
    "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
    "    def count_windows(self, grid, num_discs, piece):\n",
    "        num_windows = 0\n",
    "        # horizontal\n",
    "        for row in range(self.rows):\n",
    "            for col in range(self.columns-(self.inarow-1)):\n",
    "                window = list(grid[row, col:col+self.inarow])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "        # vertical\n",
    "        for row in range(self.rows-(self.inarow-1)):\n",
    "            for col in range(self.columns):\n",
    "                window = list(grid[row:row+self.inarow, col])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "        # positive diagonal\n",
    "        for row in range(self.rows-(self.inarow-1)):\n",
    "            for col in range(self.columns-(self.inarow-1)):\n",
    "                window = list(grid[range(row, row+self.inarow), range(col, col+self.inarow)])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "        # negative diagonal\n",
    "        for row in range(self.inarow-1, self.rows):\n",
    "            for col in range(self.columns-(self.inarow-1)):\n",
    "                window = list(grid[range(row, row-self.inarow, -1), range(col, col+self.inarow)])\n",
    "                if self.check_window(window, num_discs, piece):\n",
    "                    num_windows += 1\n",
    "                    \n",
    "        return num_windows\n",
    "    \n",
    "    def calculate_heuristic_reward(self):\n",
    "        grid = np.asarray(self.obs.board).reshape(self.rows, self.columns)\n",
    "        twos = self.count_windows(grid, 2, self.obs.mark)\n",
    "        threes = self.count_windows(grid, 3, self.obs.mark)\n",
    "        \n",
    "        reward = (twos - self.last_twos) * 1 + (threes - self.last_threes) * 3\n",
    "        if reward < 0:\n",
    "            raise AssertionError(f'\\nmark:   {self.obs.mark}   last_action: {self.last_action}\\n' + \n",
    "                                 f'twos:   {twos}   last_twos: {self.last_twos}\\n' + \n",
    "                                 f'threes: {threes} last_threes: {self.last_threes}\\n' +\n",
    "                                 f'Board:  \\n{np.array_str(grid)}')\n",
    "            \n",
    "        self.last_twos = twos\n",
    "        self.last_threes = threes\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 1\n",
    "        elif done: # The opponent won the game\n",
    "            return -1\n",
    "        else: # Reward 1/42\n",
    "            return 1/(self.rows*self.columns)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -10, True, {}\n",
    "            \n",
    "        new_board = board_flip(self.obs.mark, np.array(self.obs['board']).reshape(self.rows,self.columns,1)) \n",
    "        return new_board, reward, done, _\n",
    "    \n",
    "\n",
    "    \n",
    "\"\"\"    \n",
    "    def change_reward(self, old_reward, done):\n",
    "        if old_reward == 1: # The agent won the game\n",
    "            return 100\n",
    "        elif done: # The opponent won the game\n",
    "            return -100\n",
    "        else: # Reward 10/42\n",
    "            still_alive_reward = 10 / (self.rows*self.columns)\n",
    "            #move_reward = self.calculate_heuristic_reward()\n",
    "            return still_alive_reward# + move_reward\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Check if agent's move is valid\n",
    "        self.last_action = action\n",
    "        is_valid = (self.obs['board'][int(action)] == 0)\n",
    "        if is_valid: # Play the move\n",
    "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
    "            reward = self.change_reward(old_reward, done)\n",
    "        else: # End the game and penalize agent\n",
    "            reward, done, _ = -1000, True, {}\n",
    "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
    "\"\"\"    \n",
    "    \n",
    "    \n",
    "# Create ConnectFour environment\n",
    "env = ConnectFourGym([matrix_agent, 'random', agentc1, agentc2, agentc3, agentc5])\n",
    "\n",
    "# Create directory for logging training information\n",
    "log_dir = \"logtf1/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Logging progress\n",
    "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "# Create a vectorized environment\n",
    "vec_env = DummyVecEnv([lambda: monitor_env])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
    "from tensorflow.layers import Dropout, BatchNormalization, Dense, Conv2D\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "args = dotdict({\n",
    "    'lr': 0.001,\n",
    "    'dropout': 0.3,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64,\n",
    "    'num_channels': 64,\n",
    "})\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 64\n",
    "\n",
    "BN1 = BatchNormalization()\n",
    "BN2 = BatchNormalization()\n",
    "BN3 = BatchNormalization()\n",
    "BN4 = BatchNormalization()\n",
    "BN5 = BatchNormalization()\n",
    "BN6 = BatchNormalization()\n",
    "\n",
    "\n",
    "CONV1 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')\n",
    "CONV2 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1, padding='same')\n",
    "CONV3 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)\n",
    "CONV4 = Conv2D(NUM_CHANNELS, kernel_size=3, strides=1)\n",
    "\n",
    "FC1 = Dense(128)\n",
    "FC2 = Dense(64)\n",
    "FC3 = Dense(7)\n",
    "\n",
    "DROP1 = Dropout(0.3)\n",
    "DROP2 = Dropout(0.3)\n",
    "\n",
    "\n",
    "# 6x7 input\n",
    "# https://github.com/PaddlePaddle/PARL/blob/0915559a1dd1b9de74ddd2b261e2a4accd0cd96a/benchmark/torch/AlphaZero/submission_template.py#L496\n",
    "def modified_cnn(inputs, **kwargs):\n",
    "    relu = tf.nn.relu\n",
    "    log_softmax = tf.nn.log_softmax\n",
    "    \n",
    "    \n",
    "    layer_1_out = relu(BN1(CONV1(inputs)))\n",
    "    layer_2_out = relu(BN2(CONV2(layer_1_out)))\n",
    "    layer_3_out = relu(BN3(CONV3(layer_2_out)))\n",
    "    layer_4_out = relu(BN4(CONV4(layer_3_out)))\n",
    "    \n",
    "    # 3 is width - 4 due to convolition filters, 2 is same for height\n",
    "    flattened = tf.reshape(layer_4_out, [-1, NUM_CHANNELS * 3 * 2]) \n",
    "    \n",
    "    layer_5_out = DROP1(relu(BN5(FC1(flattened))))\n",
    "    layer_6_out = DROP2(relu(BN6(FC2(layer_5_out))))\n",
    "    \n",
    "    return log_softmax(FC3(layer_6_out))  \n",
    "\n",
    "# https://www.kaggle.com/c/connectx/discussion/128591\n",
    "class CustomCnnPolicy(CnnPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from common import get_win_percentages_and_score, serializeAndCompress\n",
    "\n",
    "class SaveBestModelCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, model_basename, save_frequency, test_agents, verbose=0):\n",
    "        super(SaveBestModelCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.step_counter = 0\n",
    "        self.best_value = -np.inf\n",
    "        self.model_basename = model_basename\n",
    "        self.save_frequency = save_frequency\n",
    "        self.test_agents = test_agents\n",
    "        \n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.step_counter += 1\n",
    "        if self.step_counter % self.save_frequency == 0:\n",
    "            print(f'Running check model check on step {self.step_counter}')\n",
    "            def trained_agent(obs, config):\n",
    "                # Use the best model to select a column\n",
    "                grid = board_flip(obs.mark, np.array(obs['board']).reshape(6,7,1))\n",
    "                col, _ = self.model.predict(grid, deterministic=True)\n",
    "                # Check if selected column is valid\n",
    "                is_valid = (obs['board'][int(col)] == 0)\n",
    "                # If not valid, select random move. \n",
    "                if is_valid:\n",
    "                    return int(col)\n",
    "                else:\n",
    "                    return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n",
    "            \n",
    "            \n",
    "            score = sum([get_win_percentages_and_score(trained_agent, test_agent, silent=True) for test_agent in self.test_agents])\n",
    "            if score > self.best_value:\n",
    "                self.best_value = score\n",
    "                print('=' * 80)\n",
    "                print(f'New best agent found with score {score}! Agent encoded:')\n",
    "                model_serialized = serializeAndCompress(self.model.get_parameters())\n",
    "                print(model_serialized)\n",
    "                with open(self.model_basename + str(self.step_counter) + '.model', 'wb') as f:\n",
    "                    f.write(model_serialized)\n",
    "                \n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, run the code cell below to train an agent with PPO and view how the rewards evolved during training.  This code is identical to the code from the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "# Try CnnPolicy and MlpPolicy\n",
    "# https://www.kaggle.com/toshikazuwatanabe/connect4-make-submission-with-stable-baselines3/comments\n",
    "\n",
    "\n",
    "eval_callback = SaveBestModelCallback('RDaneelConnect4_', 1000, ['random', agentc1, agentc3, agentc5, matrix_agent])\n",
    "\n",
    "model = PPO2(CustomCnnPolicy, vec_env, verbose=1)\n",
    "\n",
    "\n",
    "# Train agent\n",
    "model.learn(total_timesteps=30000000, callback=eval_callback)\n",
    "\n",
    "#vec_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cumulative reward\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "\n",
    "\n",
    "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
    "    firstline = fh.readline()\n",
    "    assert firstline[0] == '#'\n",
    "    df = pd.read_csv(fh, index_col=None)['r']\n",
    "df.rolling(window=100).mean().plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def dqn_agent(obs, config):\n",
    "    # Use the best model to select a column\n",
    "    grid = board_flip(obs.mark, np.array(obs['board']).reshape(6,7,1))\n",
    "    col, _ = model.predict(grid, deterministic=True)\n",
    "    # Check if selected column is valid\n",
    "    is_valid = (obs['board'][int(col)] == 0)\n",
    "    # If not valid, select random move. \n",
    "    if is_valid:\n",
    "        return int(col)\n",
    "    else:\n",
    "        grid = grid.reshape(6, 7)\n",
    "        #sleep(2)\n",
    "        #print(f'Illegal move attempted! Move: {col}, Boardf:\\n{grid}')\n",
    "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])\n",
    "    \n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.play([dqn_agent, None], width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_win_percentages_and_score('random', dqn_agent)\n",
    "get_win_percentages_and_score(agentc1, dqn_agent)\n",
    "get_win_percentages_and_score(agentc2, dqn_agent)\n",
    "get_win_percentages_and_score(agentc3, dqn_agent)\n",
    "get_win_percentages_and_score(agentc5, dqn_agent)\n",
    "get_win_percentages_and_score(agentc7, dqn_agent)\n",
    "get_win_percentages_and_score(matrix_agent, dqn_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = serializeAndCompress(model.get_parameters())\n",
    "print(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your agent trained well, the plot (which shows average cumulative rewards) should increase over time.\n",
    "\n",
    "Once you have verified that the code runs, try making amendments to see if you can get increased performance.  You might like to:\n",
    "- change `PPO1` to `A2C` (or `ACER` or `ACKTR` or `TRPO`) when defining the model in this line of code: `model = PPO1(CustomCnnPolicy, vec_env, verbose=0)`.  This will let you see how performance can be affected by changing the algorithm from Proximal Policy Optimization [PPO] to one of:\n",
    "  - Advantage Actor-Critic (A2C),\n",
    "  - or Actor-Critic with Experience Replay (ACER),\n",
    "  - Actor Critic using Kronecker-factored Trust Region (ACKTR), or \n",
    "  - Trust Region Policy Optimization (TRPO).\n",
    "- modify the `change_reward()` method in the `ConnectFourGym` class to change the rewards that the agent receives in different conditions.  You may also need to modify `self.reward_range` in the `__init__` method (this tuple should always correspond to the minimum and maximum reward that the agent can receive).\n",
    "- change `agent2` to a different agent when creating the ConnectFour environment with `env = ConnectFourGym(agent2=\"random\")`.  For instance, you might like to use the `\"negamax\"` agent, or a different, custom agent.  Note that the smarter you make the opponent, the harder it will be for your agent to train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations!\n",
    "\n",
    "You have completed the course, and it's time to put your new skills to work!  \n",
    "\n",
    "The next step is to apply what you've learned to a **[more complex game: Halite](https://www.kaggle.com/c/halite)**.  For a step-by-step tutorial in how to make your first submission to this competition, **[check out the bonus lesson](https://www.kaggle.com/alexisbcook/getting-started-with-halite)**!\n",
    "\n",
    "You can find more games as they're released on the **[Kaggle Simulations page](https://www.kaggle.com/simulations)**.\n",
    "\n",
    "As we did in the course, we recommend that you start simple, with an agent that follows your precise instructions.  This will allow you to learn more about the mechanics of the game and to build intuition for what makes a good agent.  Then, gradually increase the complexity of your agents to climb the leaderboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**[Intro to Game AI and Reinforcement Learning Home Page](https://www.kaggle.com/learn/intro-to-game-ai-and-reinforcement-learning)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS TF1 GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
